# Roffice Hour Q3: Roffice Hour Q3: How to start a PhD / New Field / New Topic

> What did you do when you started the PhD, or even before, when you're starting a new field / topic?

## [2012-2014] What I did before starting my PhD

### Searching for my role model papers

Besides getting my paper rejected from ICASSP and ISMIR, I spent quite some time reading papers and blog posts. I think I was looking for references for what I'd do.

I read [Oscar Celma's PhD Thesis](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vyV-odIAAAAJ&citation_for_view=vyV-odIAAAAJ:jSAVyFp_754C) on music recommendation, published in 2009. It was 2013, and that was perhaps when he was already working in Pandora's music recommendation team. I really studied his thesis and got a lot of motivation and knowledge. It would've also set some notion about how a usual - therefore, my - PhD thesis should be written. His thesis also became the foundation of my research plan during the admission process.

On my first day of PhD, 2014 Oct 1 (or.. 1/10/14, yes ðŸ‡¬ðŸ‡§), I read [Sander Dieleman's legendary blog post about his work at Spotify](https://sander.ai/2014/08/05/spotify-cnns.html). I read it so many times. I was more than fascinated; it shaped my initial research direction and the "initial" means like 1-2 years.. or maybe even more. I don't remember how it stood out to me from many other possible blogs and reading materials, but I guess its relevance and greatness was pretty obvious.

### Lectures

In the first months of my PhD, I took [Andrew Ng's Coursera lecture on Machine Learning](https://www.youtube.com/watch?v=jGwO_UgTS7I). It was the OG one, which was recorded around 2012.

I also watched [Geoffrey Hinton's Deep Learning lecture](https://www.youtube.com/playlist?list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv) also recorded around 2012. I learned a lot!

On these two, I tried to do the homework as diligently as possible - i.e. when it had 3 problems, I did 0.7 * question 1, 0.3 * question 2, and 0.01 * question 3. It was a huge effort compared to my default baseline (0.01 * any homework).

### Books

Books are really important! I love books, and people should read more books. Engineering, research, literature, whichever you'd choose.

I studied [Pattern Recognition and Machine Learning by Christopher Bishop](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) because my ETRI colleague [Minje Kim](https://minjekim.com) had it on his desk. I studied it while preparing for my PhD admission, in 2013, with another friend and colleague of mine, [Taejin Park](https://scholar.google.com/citations?user=ZmUDgEMAAAAJ&hl=ko). Oh yes we struggled so much.. we were so bad. Gosh, the book was so difficult. So so hard. So detailed. We also had no idea which section to focus on, which to skip, any context or whatsoever. But we probably ended up learning some concepts of ML as well as building some muscle memory on how difficult a new area can be.

### Being distracted

In my first year of PhD, I was really not sure if I should keep doing this. I considered quitting it and going back to Korea. I wanted to make iPhone apps, and I actually had made one then. I ended up making one or two more, but turned out my idea was running out; I was also not making a holistic learning on not only the development but also marketing, pricing, user understanding, or any similar sort of things. So it was not that promising either, and I was also busy being distracted by other things in my life. I continued, and later I perhaps just forgot about this distraction.

### Daydreaming with job postings

I'd open job pages of Spotify and Pandora, read the long list of required or preferred experiences and tech stacks, feeling frustrated, worried, and still excited.

### Reading Reddit: r/MachineLearning

There was time in 2014-2015 when I was reading every article and comment in [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). No one was doing deep learning in Queen Mary except one guy - [Siddarth](https://scholar.google.com/citations?user=2ulom_QAAAAJ&hl=en) and he graduated soon, and he was more experienced so he told me some stuff that I couldn't comprehend. Well.. I really learned a lot from the ML reddit. At one point though, I realized I overdid this breadth expanding. I got familiar with a lot of ML words, really a lot of them, and that's because I was insecure about my ML and felt like I know a little bit of everything. I ended up recognizing every ML word but that didn't mean.. actually, that almost always means I didn't know about anything deeper. I don't think I successfully adjusted this habit on purpose. This doom scroll-reading was perhaps replaced with reading Twitter timeline. Anyway, it was good, but it also had a side effect.

## [2023] How I started LLMs

I sort of revived the habit of reading things on random places (github issues and comments, reddit, youtube, torch QA board, ..) when I started LLMs. That's because high quality information was a bit scarce in early 2023.

There's one thing I added recently. I avoid watching YouTube videos summarizing papers for you. Those are too kind and that doesn't help, because the kindness just flows through your brain. No no, what you need is some friction, a bit of "what?" or "..what?" that will carve knowledge into your parameters. (Some videos are exceptions, such as Andrej Karpathy's ones, especially if you can code them through.)