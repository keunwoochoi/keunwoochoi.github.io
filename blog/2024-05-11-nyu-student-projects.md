# Brilliant Student Projects from NYU's Deep Learning for Media Course

Teaching "Deep Learning for Media" at NYU was super fun! It was my first time teaching a course. As other people have said, it is absolutely rewarding to communicate with those curious minds, such precious gems for people like me who are working in industry.

I'd like to share my students' final projects. These are really cool projects that showcase the creativity and technical skills of the students.

What's most impressive is how much progress they made in the final week. I swear none of these projects was at this level just one week before the deadline! üòÇ

Let me walk you through each group's project:

## Group 1: Senseflosser

By Alexander Brewer, Matthew Falgowski, and Mark Frick

They created "Senseflosser," which is, as they describe it:

> A homophonic translation of Sinnesl√∂schen, itself a non-idiomatic translation of "sense deletion", from the Polybius mythos.

The team cleverly modified a music autoencoder to mimic brain fog. It's a fascinating exploration of how neural networks can be used to create artistic experiences that simulate cognitive states.

**[Check out their project on GitHub](https://github.com/hyve9/senseflosser)**

## Group 2: Color-based Music Recommendation System

By Zhinuo Li, Duanning Wang, and Kaiyang Zhao

This group created an innovative music recommendation system that uses color palettes as an interface to find similar songs. It's a novel approach to music discovery that leverages the emotional associations we have with colors.

**[Check out their project on GitHub](https://github.com/KaiyangZhao0603/Music-Recommendation-System-Based-on-Color-Based-Emotions)**

## Group 3: Text2Map - Procedural Dungeon Generation

By Yuhe Nie and Rolly Gu

This team used a multimodal LLM in a clever way to generate game dungeon maps from text prompts. It's a great example of how AI can be used in creative applications like game design.

**[Check out their project on GitHub](https://github.com/NYH-Dolphin/DL4MT2M)**

## Group 4: Foley-Gen

By Zi Meng, Evgeny Milov, and Shaun Ranade

I was happy to see this group adopt our #DCASE2023 challenge - foley sound synthesis. They created a system that can generate realistic sound effects for visual content.

**[Check out their project on GitHub](https://github.com/zimeng44/Foley-Gen)**

## Group 5: Singing Deepfake Detection

By Yulu Fu, Mona Wang, and Deyani Herber

This group tackled the important problem of detecting deepfake singing voices. They used the SVDD2024 dataset from the [SingFake challenge](https://challenge.singfake.org). Notably, they enhanced the dataset by labeling the singing language of each sample!

**[Check out their project on GitHub](https://github.com/Monawang15/group_5_DL4M1)**

## Group 6: Speech Source Separation

By Ohm Patel, Emily Wang, and Yilin Wang

This team worked on separating mixed speech signals using the LibriMix dataset. They implemented a ConvTasNet model with Temporal Convolutional Networks (TCN) to achieve impressive separation results.

**[Check out their project on GitHub](https://github.com/ohmpatel46/DL4M_Project)**

## Group 7: Rap Vocals vs Speech Classification

By Vio Chung, Junzhe Liu, and Nick Lin

This group created a system to distinguish between rap vocals and normal speech. They explored various combinations of modules to solve this fun problem and even created their own dataset. During the demo, Nick became the first AI-proven rapper in our class!

**[Check out their project on GitHub](https://github.com/Vio-Chung/Rap-Speech-Classification/tree/main)**

## Group 8: Hand Pose and Music Control

By Tiankai Li, Lujie Wang, and Ruby Zhang

This team combined hand pose detection with music control in Max/MSP. Their system allows users to control timbre and pitch through hand gestures. They created their own dataset, implemented the system, and gave an impressive demo.

**[Check out their project on GitHub](https://github.com/RubyQianru/Hand-Pose-and-Music-Control)**

## Group 9: Finetuned Speech T5 for Spanish

By Miya Ding, Heqi Qiao, and Trevor Freed

This group finetuned the Speech T5 model for Spanish, achieving both language and speaker transfer. As they joked, now they don't even need to study Spanish!

**[Check out their project on GitHub](https://github.com/HQQHQ/FinetuneSpeechT5-Spanish)**

## Acknowledgments

I want to thank [Lightning AI](https://lightning.ai/) for providing compute credits for Lightning Studio. It was used by some of the students and made the whole process smooth for students, TAs, and myself. I highly recommend it for academic settings!

Please check out their repositories to learn more about their work. I'm incredibly proud of what these students accomplished in just one semester. 
